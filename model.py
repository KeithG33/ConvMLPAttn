import torch
import torch.nn as nn
import torch.nn.functional as F


class LayerNorm2d(nn.Module):
    """
    Layer Normalization over the channels for a 4D tensor (NCHW).
    """
    def __init__(self, num_channels: int, eps: float = 1e-6):
        super().__init__()
        self.ln = nn.LayerNorm(num_channels, eps=eps)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for LayerNorm2d.
        Args:
            x: Input tensor of shape (N, C, H, W).
        Returns:
            Normalized tensor of the same shape.
        """
        # Permute from (N, C, H, W) to (N, H, W, C) for LayerNorm
        x = x.permute(0, 2, 3, 1).contiguous()
        x = self.ln(x)
        # Permute back to (N, C, H, W)
        return x.permute(0, 3, 1, 2).contiguous()

class GRN(nn.Module):
    """ GRN (Global Response Normalization) layer
    """
    def __init__(self, dim):
        super().__init__()
        self.gamma = nn.Parameter(torch.zeros(1, 1, 1, dim))
        self.beta = nn.Parameter(torch.zeros(1, 1, 1, dim))

    def forward(self, x):
        Gx = torch.norm(x, p=2, dim=(1,2), keepdim=True)
        Nx = Gx / (Gx.mean(dim=-1, keepdim=True) + 1e-6)
        return self.gamma * (x * Nx) + self.beta + x
    

class CNV2Block(nn.Module):
    """ ConvNeXtV2 Block.
    
    Args:
        dim (int): Number of input channels.
        drop_path (float): Stochastic depth rate. Default: 0.0
    """
    def __init__(self, dim, dim_out, expand_ratio, drop_path=0., residual=None):
        super().__init__()
        self.residual = (dim == dim_out) if residual is None else residual
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv
        self.norm = nn.LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, expand_ratio * dim) # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.grn = GRN(expand_ratio * dim)
        self.pwconv2 = nn.Linear(expand_ratio * dim, dim_out)

    def forward(self, x):
        input = x
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.grn(x)
        x = self.pwconv2(x)
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)

        if self.residual:
            x = input + x
        return x


class AttnGenerator2D(nn.Module):
    """
    A 2D attention generator that generates attention logits for a 4D tensor (NCHW).
    """
    def __init__(self, N: int, C: int, heads: int = 4, kernel_size: int = 7):
        super().__init__()
        self.local_mix = nn.Sequential(
            nn.Conv2d(C, C, kernel_size=kernel_size, padding=kernel_size//2, groups=C),
            nn.GELU(),
        )
        self.global_mix = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(C, C, kernel_size=1),
            nn.GELU(),
        )
        self.mix = nn.Sequential(
            LayerNorm2d(C),
            nn.Conv2d(C, heads * N, kernel_size=1),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_local = self.local_mix(x)
        x_global = self.global_mix(x)
        x_combined = x_local + x_global
        logits = self.mix(x_combined)  # (B, h*N, H, W)
        return logits


class MLPAttn2D(nn.Module):
    """
    A 2D attention mechanism where the attention matrix is generated by an MLP.
    """
    def __init__(self, N: int, C: int, heads: int = 4, dv: int = None, attn_dropout: float = 0.0):
        super().__init__()
        assert N > 0, "N must be a positive integer."
        self.N = N
        self.C = C
        self.heads = heads
        self.dv = dv if dv is not None else (C // heads)
        assert self.heads * self.dv == C, "heads * dv must equal C."

        # self.attn_generator = CNV2Block(C, N*heads, expand, drop_path=0.)
        self.attn_generator = AttnGenerator2D(N, C, heads=heads)

        self.v_proj = nn.Sequential(
            LayerNorm2d(heads * self.dv),
            nn.Conv2d(C, heads * self.dv, kernel_size=1),
        )
        self.out_proj = nn.Sequential(
            LayerNorm2d(C),
            nn.Conv2d(heads * self.dv, C, kernel_size=1),
        )

        self.attn_drop = nn.Dropout(attn_dropout) if attn_dropout > 0 else nn.Identity()
        self.logit_scale = nn.Parameter(torch.tensor(1.0))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, H, W = x.shape
        assert H * W == self.N, "Input feature map size does not match N."

        # Generate attention logits
        logits = self.attn_generator(x)  # (B, h*N, H, W)
        attn = logits.view(B, self.heads, self.N, self.N)  # (B, h, N, N)
         
        attn = attn * self.logit_scale
        attn = F.softmax(attn, dim=-1)
        attn = self.attn_drop(attn)

        v = self.v_proj(x)
        v = v.view(B, self.heads, self.dv, self.N).transpose(-2, -1)  # (B, h, N, dv)

        y = torch.matmul(attn, v)  # (B, h, N, dv)
        y = y.transpose(-2, -1).reshape(B, self.heads * self.dv, H, W)  # (B, C, H, W)
        return self.out_proj(y)


class AttnMLPBlock2D(nn.Module):
    """
    A standard transformer-style block with two residual connections:
    1. Attention block: x + Attn(LN(x))
    2. MLP block: x + MLP(LN(x))
    """
    def __init__(self, N: int, C: int, heads: int, dv: int = None, attn_dropout: float = 0.0, mlp_expand: int = 4):
        super().__init__()
        self.norm1 = LayerNorm2d(C)
        self.attn = MLPAttn2D(N=N, C=C, heads=heads, dv=dv, attn_dropout=attn_dropout)
        self.norm2 = LayerNorm2d(C)
        self.mlp = nn.Sequential(
            nn.Conv2d(C, C * mlp_expand, kernel_size=1),
            nn.GELU(),
            nn.Conv2d(C * mlp_expand, C, kernel_size=1),
        )
        # self.mlp = CNV2Block(C, C, expand_ratio=mlp_expand, residual=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.attn(self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

class DownsampleLayer(nn.Module):
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.net = nn.Sequential(
            LayerNorm2d(in_channels), 
            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1) 
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


class Stage(nn.Module):
    def __init__(self, size_hw: tuple, C: int, depth: int, heads: int, block_type='conv', dp_rates=[0.], mlp_ratio=4.):
        super().__init__()
        H, W = size_hw
        N = H * W
        layers = []
        for i in range(depth):
            if block_type == 'conv':
                layers.append(CNV2Block(C, C, expand_ratio=mlp_ratio))
            else:
                layers.append(AttnMLPBlock2D(N=N, C=C, heads=heads, mlp_expand=mlp_ratio))
        self.blocks = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.blocks(x)


class MLPAttn2DNet(nn.Module):
    """
    The main network architecture.
    It processes an image through a patch embedding layer, followed by four stages
    of AttnMLP blocks. Downsampling occurs between stages. The final output is
    produced by a classification head.
    """
    def __init__(self, img_size=224, in_chans=3, num_classes=1000, 
                 depths=(2, 2, 8, 2), 
                 dims=(48, 96, 192, 384), 
                 heads=(0, 0, 4, 8), 
                 drop_path_rate=0.0,
                 mlp_expand=2):
        super().__init__()
        assert img_size % 32 == 0, "Image size must be divisible by 32."
       
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        
        # Feature map sizes at each stage
        s1 = img_size // 4
        s2 = s1 // 2
        s3 = s2 // 2
        s4 = s3 // 1

        C1, C2, C3, C4 = dims

        for c, h in zip(dims, heads):
            if h > 0:
                assert c % h == 0, "Dimension must be divisible by the number of heads."

        self.patch_embed = nn.Sequential(
            nn.Conv2d(in_chans, dims[0] // 2, kernel_size=3, stride=2, padding=1),
            LayerNorm2d(dims[0] // 2),
            nn.GELU(),
            nn.Conv2d(dims[0] // 2, dims[0], kernel_size=3, stride=2, padding=1),
            LayerNorm2d(dims[0]),
        )

        # Stage 1 (ConvNeXt V2)
        self.stage1 = Stage((s1, s1), C=dims[0], depth=depths[0], heads=heads[0], block_type='conv', 
                            mlp_ratio=mlp_expand)
        self.down12 = DownsampleLayer(dims[0], dims[1])
        
        # Stage 2 (ConvNeXt V2)
        self.stage2 = Stage((s2, s2), C=dims[1], depth=depths[1], heads=heads[1], block_type='conv', 
                            mlp_ratio=mlp_expand)
        self.down23 = DownsampleLayer(dims[1], dims[2])
        
        # Stage 3 (MLP Attn)
        self.stage3 = Stage((s3, s3), C=dims[2], depth=depths[2], heads=heads[2], block_type='attn', 
                            mlp_ratio=mlp_expand)
        # self.down34 = DownsampleLayer(dims[2], dims[3])
        self.down34 = nn.Sequential(
            LayerNorm2d(C3),
            nn.Conv2d(C3, C4, kernel_size=1),
        )
        
        # Stage 4 (MLP Attn)
        self.stage4 = Stage((s4, s4), C=dims[3], depth=depths[3], heads=heads[3], block_type='attn', 
                            mlp_ratio=mlp_expand)


        # Classification Head
        self.norm_head = LayerNorm2d(C4)
        self.head = nn.Linear(C4, num_classes)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.patch_embed(x)
        
        x = self.stage1(x)
        x = self.down12(x)
        
        x = self.stage2(x)
        x = self.down23(x)
        
        x = self.stage3(x)
        x = self.down34(x)
        
        x = self.stage4(x)
        x = self.norm_head(x)
        
        x = F.adaptive_avg_pool2d(x, 1).flatten(1)
        return self.head(x)


# --- Quick Sanity Test ---
if __name__ == "__main__":
    model = MLPAttn2DNet(
        depths=(2, 2, 6, 3),
        dims=(48, 96, 128, 224),
        heads=(2, 2, 4, 8),
        mlp_expand=2,
    )

    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Number of parameters: {num_params / 1e6:.2f}M")
    
    dummy_input = torch.randn(1, 3, 224, 224)
    with torch.no_grad():
        logits = model(dummy_input)
    print(f"Output logits shape: {logits.shape}")

    # benchmark speed
    import time
    repetitions = 100
    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)
    dummy_input = dummy_input.cuda().to(dtype=torch.bfloat16)
    model = model.cuda().to(dtype=torch.bfloat16)
    model.eval()

    with torch.inference_mode():
            
        for _ in range(10):
            _ = model(dummy_input)
        
        torch.cuda.synchronize()
        
        starter.record()
        for _ in range(repetitions):
            _ = model(dummy_input)
        ender.record()

        torch.cuda.synchronize()
        avg_ms = starter.elapsed_time(ender) / repetitions
    
    print(f"Average inference time: {avg_ms:.2f} ms")